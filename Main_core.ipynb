{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sabri\\Desktop\\GENAI Hackathon\\my_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from datasets import load_dataset, DatasetDict\n",
    "from huggingface_hub import login\n",
    "import re\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU available: NVIDIA GeForce RTX 4060 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "assert torch.cuda.get_device_capability()[0] >= 8, 'Hardware not supported for Flash Attention'\n",
    "if torch.cuda.is_available():\n",
    "    print(\"GPU available:\", torch.cuda.get_device_name(0))\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    print(\"No GPU detected\")\n",
    "    device = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daily_dialog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 4.48M/4.48M [00:00<00:00, 5.57MB/s]\n",
      "Generating train split: 100%|██████████| 11118/11118 [00:00<00:00, 12958.66 examples/s]\n",
      "Generating validation split: 100%|██████████| 1000/1000 [00:00<00:00, 10390.58 examples/s]\n",
      "Generating test split: 100%|██████████| 1000/1000 [00:00<00:00, 10409.38 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['dialog', 'act', 'emotion'],\n",
       "        num_rows: 11118\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['dialog', 'act', 'emotion'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['dialog', 'act', 'emotion'],\n",
       "        num_rows: 1000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"daily_dialog\", trust_remote_code=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialog': ['Say , Jim , how about going for a few beers after dinner ? ',\n",
       "  ' You know that is tempting but is really not good for our fitness . ',\n",
       "  ' What do you mean ? It will help us to relax . ',\n",
       "  \" Do you really think so ? I don't . It will just make us fat and act silly . Remember last time ? \",\n",
       "  \" I guess you are right.But what shall we do ? I don't feel like sitting at home . \",\n",
       "  ' I suggest a walk over to the gym where we can play singsong and meet some of our friends . ',\n",
       "  \" That's a good idea . I hear Mary and Sally often go there to play pingpong.Perhaps we can make a foursome with them . \",\n",
       "  ' Sounds great to me ! If they are willing , we could ask them to go dancing with us.That is excellent exercise and fun , too . ',\n",
       "  \" Good.Let ' s go now . \",\n",
       "  ' All right . '],\n",
       " 'act': [3, 4, 2, 2, 2, 3, 4, 1, 3, 4],\n",
       " 'emotion': [0, 0, 0, 0, 0, 0, 4, 4, 4, 4]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Empathetic_dialogue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Situation</th>\n",
       "      <th>emotion</th>\n",
       "      <th>empathetic_dialogues</th>\n",
       "      <th>labels</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :I remember going to see the firework...</td>\n",
       "      <td>Was this a friend you were in love with, or ju...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :This was a best friend. I miss her.\\...</td>\n",
       "      <td>Where has she gone?</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :We no longer talk.\\nAgent :</td>\n",
       "      <td>Oh was this something that happened because of...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :Was this a friend you were in love w...</td>\n",
       "      <td>This was a best friend. I miss her.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>I remember going to the fireworks with my best...</td>\n",
       "      <td>sentimental</td>\n",
       "      <td>Customer :Where has she gone?\\nAgent :</td>\n",
       "      <td>We no longer talk.</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                          Situation      emotion  \\\n",
       "0           0  I remember going to the fireworks with my best...  sentimental   \n",
       "1           1  I remember going to the fireworks with my best...  sentimental   \n",
       "2           2  I remember going to the fireworks with my best...  sentimental   \n",
       "3           3  I remember going to the fireworks with my best...  sentimental   \n",
       "4           4  I remember going to the fireworks with my best...  sentimental   \n",
       "\n",
       "                                empathetic_dialogues  \\\n",
       "0  Customer :I remember going to see the firework...   \n",
       "1  Customer :This was a best friend. I miss her.\\...   \n",
       "2              Customer :We no longer talk.\\nAgent :   \n",
       "3  Customer :Was this a friend you were in love w...   \n",
       "4             Customer :Where has she gone?\\nAgent :   \n",
       "\n",
       "                                              labels Unnamed: 5 Unnamed: 6  \n",
       "0  Was this a friend you were in love with, or ju...        NaN        NaN  \n",
       "1                                Where has she gone?        NaN        NaN  \n",
       "2  Oh was this something that happened because of...        NaN        NaN  \n",
       "3                This was a best friend. I miss her.        NaN        NaN  \n",
       "4                                 We no longer talk.        NaN        NaN  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"empathetic-dialogues-facebook-ai/emotion-emotion_69k.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing the Customer:, and agent: part from the 'empethetic_dialogues' column\n",
    "df['extracted_text'] = df['empathetic_dialogues'].apply(lambda x: re.findall(r':\\s*(.*?)\\n', x)[0] if re.findall(r':\\s*(.*?)\\n', x) else None)\n",
    "# keeping only the extracted_text, and labels columns\n",
    "cleaned_df = df[[\"extracted_text\", \"labels\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 1419,  1420,  1421,  1422,  2546,  2547,  2548,  3722,  3723,  3724,\n",
       "        3725,  3726, 20933, 20934, 20935, 20936, 23485, 23486, 23487, 23488,\n",
       "       28674, 28675, 28676, 31743, 31744, 31745, 35867, 35868, 35869, 35870,\n",
       "       35871, 40194, 40195, 40196, 40197, 42140, 42141, 42142, 53778, 53779,\n",
       "       53780, 53781, 64217, 64218, 64219],\n",
       "      dtype='int64')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# these rows have parsing problems\n",
    "cleaned_df[cleaned_df.isna().any(axis=1)].index\n",
    "# these indexes are special case : [23485, 23486, 23487, 23488]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the rows with the parsing problem\n",
    "cleaned_df.drop(index=cleaned_df[cleaned_df.isna().any(axis=1)].index, inplace=True, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is for the correction of the first subrows with parsing problems\n",
    "df_v1 = df.iloc[[1419,  1420,  1421,  1422,  2546,  2547,  2548,  3722,  3723,  3724, 3725,  3726, 20933, 20934, 20935, 20936, 28674, 28675, \n",
    "                 28676, 31743, 31744, 31745, 35867, 35868, 35869, 35870, 35871, 40194, 40195, 40196, 40197, 42140, 42141, 42142, 53778, 53779,\n",
    "                 53780, 53781, 64217, 64218, 64219]].copy()\n",
    "df_v1['extracted_text'] = df_v1['labels'].apply(lambda x: re.findall(r':\\s*(.*?)\\n', x)[0] if re.findall(r':\\s*(.*?)\\n', x) else None)\n",
    "df_v1['labels'] = df_v1[\"Unnamed: 5\"]\n",
    "# And this is for the seconde subrows\n",
    "df_v2 = df.iloc[[23485, 23486, 23487, 23488]].copy()\n",
    "df_v2['extracted_text'] = df_v2['Unnamed: 5'].apply(lambda x: re.findall(r':\\s*(.*?)\\n', x)[0] if re.findall(r':\\s*(.*?)\\n', x) else None)\n",
    "df_v2['labels'] = df_v2[\"Unnamed: 6\"]\n",
    "# Then we reconcat all of the data\n",
    "cleaned_df = pd.concat([cleaned_df,df_v1[[\"extracted_text\", \"labels\"]], df_v2[[\"extracted_text\", \"labels\"]]], axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>extracted_text</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1423</th>\n",
       "      <td>One night, i hugged my wife and told her i lov...</td>\n",
       "      <td>Aww! That's so sweet! How long have you been m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1424</th>\n",
       "      <td>I went to work and got a big raise. I rushed h...</td>\n",
       "      <td>That's very exciting! Congratulations on your ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1425</th>\n",
       "      <td>Aww! That's so sweet! How long have you been m...</td>\n",
       "      <td>I went to work and got a big raise. I rushed h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1426</th>\n",
       "      <td>That's very exciting! Congratulations on your ...</td>\n",
       "      <td>When i got home, all of the furniture, my wife...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2553</th>\n",
       "      <td>High school sweethearts, that's so special. Do...</td>\n",
       "      <td>No not at all, I feel so so lucky that I met h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2554</th>\n",
       "      <td>Your outlook on love is really refreshing. I w...</td>\n",
       "      <td>Thank you very much, I'm wishing/praying the b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2555</th>\n",
       "      <td>How can people be stupid enough to step on you...</td>\n",
       "      <td>That is a disgrace. Some people just do not ge...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3734</th>\n",
       "      <td>It had it's ups and downs which is why I wonde...</td>\n",
       "      <td>Do you have kids?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3735</th>\n",
       "      <td>I do, which is one of the reasons I'm so excited.</td>\n",
       "      <td>It's never to late to start new. You guys will...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3736</th>\n",
       "      <td>Was it a nice one?</td>\n",
       "      <td>It had it's ups and downs which is why I wonde...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3737</th>\n",
       "      <td>Do you have kids?</td>\n",
       "      <td>I do, which is one of the reasons I'm so excited.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3738</th>\n",
       "      <td>I have been ordering out for the past two weeks.</td>\n",
       "      <td>Is it too hot to cook or do you just hate cook...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20949</th>\n",
       "      <td>Pretty envious that my friend can go out and b...</td>\n",
       "      <td>Ohhh gotta hate that friend!   What do you drive?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20950</th>\n",
       "      <td>An old beat up car that I need to replace but ...</td>\n",
       "      <td>I drive a gianormous conversion van</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20951</th>\n",
       "      <td>Ohhh gotta hate that friend!   What do you drive?</td>\n",
       "      <td>An old beat up car that I need to replace but ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20952</th>\n",
       "      <td>Ahhhh I wish I was 18 again.. Things were so d...</td>\n",
       "      <td>In what ways do you mean?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23505</th>\n",
       "      <td>thats awesome. dont spoil anything, okay? :D</td>\n",
       "      <td>I wo't I am sad that it is ending</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23506</th>\n",
       "      <td>well, true. me too</td>\n",
       "      <td>I cant wait for the final episode</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23507</th>\n",
       "      <td>I took this calculus test last semester and I ...</td>\n",
       "      <td>oh that sucks.. sorry about that. but you can ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23508</th>\n",
       "      <td>Yeah that's true, but I just thought I did so ...</td>\n",
       "      <td>I can feel your emotion.. dont worry, it happe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28697</th>\n",
       "      <td>Yep. Then there's graduation and prom in may.</td>\n",
       "      <td>And in between football games and soccer games...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28698</th>\n",
       "      <td>So true! I wish my phone would just do it auto...</td>\n",
       "      <td>I know Google Photos has a little reminder thi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28699</th>\n",
       "      <td>That's my favorite part of facebook!  Especial...</td>\n",
       "      <td>I know right!  I love it!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31769</th>\n",
       "      <td>It seems to be happening a lot because my best...</td>\n",
       "      <td>Stay strong! Hopefully in the future she can h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31770</th>\n",
       "      <td>I'm so sorry to hear that! My sister went thro...</td>\n",
       "      <td>It seems to be happening a lot because my best...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31771</th>\n",
       "      <td>My wife went on a business trip for two weeks....</td>\n",
       "      <td>Aw, that's never fun.  My husband used to trav...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35898</th>\n",
       "      <td>I think so, I owuld like a better job but that...</td>\n",
       "      <td>Well that's good. IT will come it you work for...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35899</th>\n",
       "      <td>Are you happy now?</td>\n",
       "      <td>I think so, I owuld like a better job but that...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35900</th>\n",
       "      <td>Three is a good number of kids for me</td>\n",
       "      <td>How many do you have now?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35901</th>\n",
       "      <td>Three. I think I am done lol</td>\n",
       "      <td>Oh, well there you go, good choice!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35902</th>\n",
       "      <td>How many do you have now?</td>\n",
       "      <td>Three. I think I am done lol</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40229</th>\n",
       "      <td>That's a good spot then</td>\n",
       "      <td>Indeed. Turns out we were still paying for ope...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40230</th>\n",
       "      <td>The sun is shining here and I have the day to ...</td>\n",
       "      <td>Must be nice, planning anything exciting?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40231</th>\n",
       "      <td>No - I think I'll go a walk and come home to w...</td>\n",
       "      <td>Thats fair, so long as you destress and have a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40232</th>\n",
       "      <td>Exactly - a nice day of chillaxing.</td>\n",
       "      <td>But you know, if you have some time off might ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42178</th>\n",
       "      <td>I'm so jealous of my friend's new girlfriend. ...</td>\n",
       "      <td>The perfect person for you will come soon enough</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42179</th>\n",
       "      <td>Hopefully, maybe I need to focus on myself mor...</td>\n",
       "      <td>Stay positive!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42180</th>\n",
       "      <td>The perfect person for you will come soon enough</td>\n",
       "      <td>Hopefully, maybe I need to focus on myself mor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53820</th>\n",
       "      <td>Have you ever invited someone over for dinner ...</td>\n",
       "      <td>Oh, no! What happened?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53821</th>\n",
       "      <td>Well I made them dinner fixed their plate gave...</td>\n",
       "      <td>Oh wow! That's like the worst nightmare! I'm s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53822</th>\n",
       "      <td>Oh, no! What happened?</td>\n",
       "      <td>Well I made them dinner fixed their plate gave...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53823</th>\n",
       "      <td>once i pooped my pants at walmart</td>\n",
       "      <td>Were you able to get out of there without anyb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64262</th>\n",
       "      <td>Well, no one was sitting immediately right bes...</td>\n",
       "      <td>That is great well know you know what not to d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64263</th>\n",
       "      <td>That is funny i feel like someone should have ...</td>\n",
       "      <td>Well, no one was sitting immediately right bes...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64264</th>\n",
       "      <td>my cat keeps bting me!</td>\n",
       "      <td>How long have you had it?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          extracted_text  \\\n",
       "1423   One night, i hugged my wife and told her i lov...   \n",
       "1424   I went to work and got a big raise. I rushed h...   \n",
       "1425   Aww! That's so sweet! How long have you been m...   \n",
       "1426   That's very exciting! Congratulations on your ...   \n",
       "2553   High school sweethearts, that's so special. Do...   \n",
       "2554   Your outlook on love is really refreshing. I w...   \n",
       "2555   How can people be stupid enough to step on you...   \n",
       "3734   It had it's ups and downs which is why I wonde...   \n",
       "3735   I do, which is one of the reasons I'm so excited.   \n",
       "3736                                  Was it a nice one?   \n",
       "3737                                   Do you have kids?   \n",
       "3738    I have been ordering out for the past two weeks.   \n",
       "20949  Pretty envious that my friend can go out and b...   \n",
       "20950  An old beat up car that I need to replace but ...   \n",
       "20951  Ohhh gotta hate that friend!   What do you drive?   \n",
       "20952  Ahhhh I wish I was 18 again.. Things were so d...   \n",
       "23505       thats awesome. dont spoil anything, okay? :D   \n",
       "23506                                 well, true. me too   \n",
       "23507  I took this calculus test last semester and I ...   \n",
       "23508  Yeah that's true, but I just thought I did so ...   \n",
       "28697     Yep. Then there's graduation and prom in may.    \n",
       "28698  So true! I wish my phone would just do it auto...   \n",
       "28699  That's my favorite part of facebook!  Especial...   \n",
       "31769  It seems to be happening a lot because my best...   \n",
       "31770  I'm so sorry to hear that! My sister went thro...   \n",
       "31771  My wife went on a business trip for two weeks....   \n",
       "35898  I think so, I owuld like a better job but that...   \n",
       "35899                                 Are you happy now?   \n",
       "35900              Three is a good number of kids for me   \n",
       "35901                       Three. I think I am done lol   \n",
       "35902                          How many do you have now?   \n",
       "40229                            That's a good spot then   \n",
       "40230  The sun is shining here and I have the day to ...   \n",
       "40231  No - I think I'll go a walk and come home to w...   \n",
       "40232                Exactly - a nice day of chillaxing.   \n",
       "42178  I'm so jealous of my friend's new girlfriend. ...   \n",
       "42179  Hopefully, maybe I need to focus on myself mor...   \n",
       "42180   The perfect person for you will come soon enough   \n",
       "53820  Have you ever invited someone over for dinner ...   \n",
       "53821  Well I made them dinner fixed their plate gave...   \n",
       "53822                             Oh, no! What happened?   \n",
       "53823                  once i pooped my pants at walmart   \n",
       "64262  Well, no one was sitting immediately right bes...   \n",
       "64263  That is funny i feel like someone should have ...   \n",
       "64264                             my cat keeps bting me!   \n",
       "\n",
       "                                                  labels  \n",
       "1423   Aww! That's so sweet! How long have you been m...  \n",
       "1424   That's very exciting! Congratulations on your ...  \n",
       "1425   I went to work and got a big raise. I rushed h...  \n",
       "1426   When i got home, all of the furniture, my wife...  \n",
       "2553   No not at all, I feel so so lucky that I met h...  \n",
       "2554   Thank you very much, I'm wishing/praying the b...  \n",
       "2555   That is a disgrace. Some people just do not ge...  \n",
       "3734                                   Do you have kids?  \n",
       "3735   It's never to late to start new. You guys will...  \n",
       "3736   It had it's ups and downs which is why I wonde...  \n",
       "3737   I do, which is one of the reasons I'm so excited.  \n",
       "3738   Is it too hot to cook or do you just hate cook...  \n",
       "20949  Ohhh gotta hate that friend!   What do you drive?  \n",
       "20950                I drive a gianormous conversion van  \n",
       "20951  An old beat up car that I need to replace but ...  \n",
       "20952                          In what ways do you mean?  \n",
       "23505                  I wo't I am sad that it is ending  \n",
       "23506                  I cant wait for the final episode  \n",
       "23507  oh that sucks.. sorry about that. but you can ...  \n",
       "23508  I can feel your emotion.. dont worry, it happe...  \n",
       "28697  And in between football games and soccer games...  \n",
       "28698  I know Google Photos has a little reminder thi...  \n",
       "28699                          I know right!  I love it!  \n",
       "31769  Stay strong! Hopefully in the future she can h...  \n",
       "31770  It seems to be happening a lot because my best...  \n",
       "31771  Aw, that's never fun.  My husband used to trav...  \n",
       "35898  Well that's good. IT will come it you work for...  \n",
       "35899  I think so, I owuld like a better job but that...  \n",
       "35900                          How many do you have now?  \n",
       "35901                Oh, well there you go, good choice!  \n",
       "35902                       Three. I think I am done lol  \n",
       "40229  Indeed. Turns out we were still paying for ope...  \n",
       "40230          Must be nice, planning anything exciting?  \n",
       "40231  Thats fair, so long as you destress and have a...  \n",
       "40232  But you know, if you have some time off might ...  \n",
       "42178   The perfect person for you will come soon enough  \n",
       "42179                                     Stay positive!  \n",
       "42180  Hopefully, maybe I need to focus on myself mor...  \n",
       "53820                             Oh, no! What happened?  \n",
       "53821  Oh wow! That's like the worst nightmare! I'm s...  \n",
       "53822  Well I made them dinner fixed their plate gave...  \n",
       "53823  Were you able to get out of there without anyb...  \n",
       "64262  That is great well know you know what not to d...  \n",
       "64263  Well, no one was sitting immediately right bes...  \n",
       "64264                          How long have you had it?  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And now we check. Voila!\n",
    "cleaned_df.iloc[[ 1419,  1420,  1421,  1422,  2546,  2547,  2548,  3722,  3723,  3724,\n",
    "        3725,  3726, 20933, 20934, 20935, 20936, 23485, 23486, 23487, 23488,\n",
    "       28674, 28675, 28676, 31743, 31744, 31745, 35867, 35868, 35869, 35870,\n",
    "       35871, 40194, 40195, 40196, 40197, 42140, 42141, 42142, 53778, 53779,\n",
    "       53780, 53781, 64217, 64218, 64219]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login(\n",
    "  token=\"--Add your token here--\", # ADD YOUR TOKEN HERE\n",
    "  # add_to_git_credential=True\n",
    ")\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'You are an emotional conversationalist. Users will talk to you in English and you will generate a response.', 'role': 'system'}, {'content': \"Yep I'm really lucky. He's always been very trustworthy and never takes advantage when I let him use it.\", 'role': 'user'}, {'content': \"That's so rare in teenagers! Sounds like you did a great job raising him.\", 'role': 'assistant'}]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating json from Arrow format: 100%|██████████| 52/52 [00:01<00:00, 36.33ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 40.79ba/s]\n",
      "Creating json from Arrow format: 100%|██████████| 7/7 [00:00<00:00, 40.76ba/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2241077"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert dataset to OAI messages\n",
    "system_message = \"\"\"You are an emotional conversationalist. Users will talk to you in English and you will generate a response.\"\"\"\n",
    " \n",
    "def create_conversation(sample):\n",
    "  return {\n",
    "    \"messages\": [\n",
    "      {\"role\": \"system\", \"content\": system_message},\n",
    "      {\"role\": \"user\", \"content\": sample[\"extracted_text\"]},\n",
    "      {\"role\": \"assistant\", \"content\": sample[\"labels\"]}\n",
    "    ]\n",
    "  }\n",
    " \n",
    "# Load dataset from the hub\n",
    "dataset = load_dataset(\"csv\", data_files=\"Data/cleaned_empethatic_dataset.csv\", split=\"train\")\n",
    "#dataset = dataset.shuffle().select(range(12500))\n",
    " \n",
    "# Convert dataset to OAI messages\n",
    "dataset = dataset.map(create_conversation, remove_columns=dataset.features,batched=False)\n",
    "# split dataset into 10,000 training samples and 2,500 test samples\n",
    "dataset_1 = dataset.train_test_split(test_size=0.2)\n",
    "dataset_2 = dataset_1[\"test\"].train_test_split(test_size=0.5)\n",
    "\n",
    "split_dataset = {\n",
    "    'train': dataset_1['train'],\n",
    "    'validation': dataset_2['train'],\n",
    "    'test': dataset_2['test']\n",
    "}\n",
    "\n",
    "new_dataset = DatasetDict(split_dataset)\n",
    "\n",
    " \n",
    "print(new_dataset[\"train\"][345][\"messages\"])\n",
    " \n",
    "# save datasets to disk\n",
    "new_dataset[\"train\"].to_json(\"Data/train_dataset.json\", orient=\"records\")\n",
    "new_dataset[\"validation\"].to_json(\"Data/validation_dataset.json\", orient=\"records\")\n",
    "new_dataset[\"test\"].to_json(\"Data/test_dataset.json\", orient=\"records\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from trl import setup_chat_format\n",
    "\n",
    "model_id = \"Qwen/Qwen1.5-0.5B-Chat\" # or `mistralai/Mistral-7B-v0.1`\n",
    " \n",
    "# BitsAndBytesConfig int-4 config\n",
    "\"\"\" bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\", \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ") \"\"\"\n",
    " \n",
    "# Load model and tokenizer\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    device_map=\"auto\",\n",
    "    #attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    #quantization_config=bnb_config\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.padding_side = 'right' # to prevent warnings\n",
    " \n",
    "# # set chat template to OAI chatML, remove if you start from a fine-tuned model\n",
    "#model, tokenizer = setup_chat_format(model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load jsonl data from disk\n",
    "dataset = load_dataset(\"json\", data_files=\"Data/train_dataset.json\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig\n",
    " \n",
    "# LoRA config based on QLoRA paper & Sebastian Raschka experiment\n",
    "peft_config = LoraConfig(\n",
    "        lora_alpha=128,\n",
    "        lora_dropout=0.05,\n",
    "        r=256,\n",
    "        bias=\"none\",\n",
    "        target_modules=\"all-linear\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    " \n",
    "args = TrainingArguments(\n",
    "    output_dir=\"code-Qwen1.5\", # directory to save and repository id\n",
    "    num_train_epochs=1,                     # number of training epochs\n",
    "    per_device_train_batch_size=3,          # batch size per device during training\n",
    "    gradient_accumulation_steps=3,          # number of steps before performing a backward/update pass\n",
    "    gradient_checkpointing=True,            # use gradient checkpointing to save memory\n",
    "    optim=\"adamw_torch_fused\",              # use fused adamw optimizer\n",
    "    logging_steps=10,                       # log every 10 steps\n",
    "    save_strategy=\"epoch\",                  # save checkpoint every epoch\n",
    "    learning_rate=2e-4,                     # learning rate, based on QLoRA paper\n",
    "    bf16=True,                              # use bfloat16 precision\n",
    "    tf32=True,                              # use tf32 precision\n",
    "    max_grad_norm=0.3,                      # max gradient norm based on QLoRA paper\n",
    "    warmup_ratio=0.03,                      # warmup ratio based on QLoRA paper\n",
    "    lr_scheduler_type=\"constant\",           # use constant learning rate scheduler\n",
    "    # push_to_hub=True,                       # push model to hub\n",
    "    report_to=\"tensorboard\",                # report metrics to tensorboard\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\sabri\\Desktop\\GENAI Hackathon\\my_env\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sabri\\Desktop\\GENAI Hackathon\\my_env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, packing, dataset_kwargs. Will not be supported from version '0.13.0'.\n",
      "\n",
      "Deprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n",
      "  warnings.warn(message, FutureWarning)\n",
      "c:\\Users\\sabri\\Desktop\\GENAI Hackathon\\my_env\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:212: UserWarning: You passed a `packing` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sabri\\Desktop\\GENAI Hackathon\\my_env\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sabri\\Desktop\\GENAI Hackathon\\my_env\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:334: UserWarning: You passed a `dataset_kwargs` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n",
      "  warnings.warn(\n",
      "Generating train split: 6957 examples [00:09, 708.76 examples/s] \n"
     ]
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    " \n",
    "max_seq_length = 512 # max sequence length for model and packing of the dataset\n",
    " \n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    peft_config=peft_config,\n",
    "    max_seq_length=max_seq_length,\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True,\n",
    "    dataset_kwargs={\n",
    "        \"add_special_tokens\": False,  # We template with special tokens\n",
    "        \"append_concat_token\": False, # No need to add additional separator token\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/773 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "  1%|▏         | 10/773 [00:15<18:31,  1.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 2.1828, 'grad_norm': 0.6152185797691345, 'learning_rate': 0.0002, 'epoch': 0.01}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 20/773 [00:29<18:08,  1.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.6085, 'grad_norm': 0.2858924865722656, 'learning_rate': 0.0002, 'epoch': 0.03}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 30/773 [00:43<17:14,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.567, 'grad_norm': 0.28052735328674316, 'learning_rate': 0.0002, 'epoch': 0.04}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 40/773 [01:00<20:33,  1.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5208, 'grad_norm': 0.2459777444601059, 'learning_rate': 0.0002, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▋         | 50/773 [01:15<17:14,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5054, 'grad_norm': 0.27517563104629517, 'learning_rate': 0.0002, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 60/773 [01:29<16:56,  1.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5229, 'grad_norm': 0.2119239717721939, 'learning_rate': 0.0002, 'epoch': 0.08}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 70/773 [01:43<16:29,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4709, 'grad_norm': 0.23544570803642273, 'learning_rate': 0.0002, 'epoch': 0.09}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 80/773 [01:57<16:37,  1.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4936, 'grad_norm': 0.23373468220233917, 'learning_rate': 0.0002, 'epoch': 0.1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 90/773 [02:12<16:06,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4826, 'grad_norm': 0.2512829303741455, 'learning_rate': 0.0002, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 100/773 [02:26<15:45,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4952, 'grad_norm': 0.22206951677799225, 'learning_rate': 0.0002, 'epoch': 0.13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 110/773 [02:40<15:39,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4691, 'grad_norm': 0.21872612833976746, 'learning_rate': 0.0002, 'epoch': 0.14}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 120/773 [02:54<15:17,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4437, 'grad_norm': 0.23098516464233398, 'learning_rate': 0.0002, 'epoch': 0.16}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 130/773 [03:08<15:00,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.5009, 'grad_norm': 0.26446661353111267, 'learning_rate': 0.0002, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 18%|█▊        | 140/773 [03:22<14:48,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.485, 'grad_norm': 0.2837963104248047, 'learning_rate': 0.0002, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 19%|█▉        | 150/773 [03:36<14:32,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.479, 'grad_norm': 0.2397577464580536, 'learning_rate': 0.0002, 'epoch': 0.19}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 21%|██        | 160/773 [03:50<14:25,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4371, 'grad_norm': 0.2557121515274048, 'learning_rate': 0.0002, 'epoch': 0.21}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 170/773 [04:04<14:01,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4508, 'grad_norm': 0.2323865443468094, 'learning_rate': 0.0002, 'epoch': 0.22}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 180/773 [04:18<13:53,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4878, 'grad_norm': 0.22693663835525513, 'learning_rate': 0.0002, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▍       | 190/773 [04:32<13:34,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.456, 'grad_norm': 0.21417443454265594, 'learning_rate': 0.0002, 'epoch': 0.25}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 200/773 [04:46<13:20,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4609, 'grad_norm': 0.24707169830799103, 'learning_rate': 0.0002, 'epoch': 0.26}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 210/773 [05:00<13:02,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4428, 'grad_norm': 0.223281592130661, 'learning_rate': 0.0002, 'epoch': 0.27}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 28%|██▊       | 220/773 [05:14<12:57,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4426, 'grad_norm': 0.23084834218025208, 'learning_rate': 0.0002, 'epoch': 0.28}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|██▉       | 230/773 [05:28<12:36,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.438, 'grad_norm': 0.29445716738700867, 'learning_rate': 0.0002, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 31%|███       | 240/773 [05:42<12:28,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.444, 'grad_norm': 0.21922005712985992, 'learning_rate': 0.0002, 'epoch': 0.31}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 250/773 [05:56<12:16,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4548, 'grad_norm': 0.26009950041770935, 'learning_rate': 0.0002, 'epoch': 0.32}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 34%|███▎      | 260/773 [06:11<12:08,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4483, 'grad_norm': 0.2541850805282593, 'learning_rate': 0.0002, 'epoch': 0.34}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▍      | 270/773 [06:25<11:41,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4364, 'grad_norm': 0.24903592467308044, 'learning_rate': 0.0002, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▌      | 280/773 [06:39<11:39,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4562, 'grad_norm': 0.2506919503211975, 'learning_rate': 0.0002, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 38%|███▊      | 290/773 [06:53<11:18,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.452, 'grad_norm': 0.2379612922668457, 'learning_rate': 0.0002, 'epoch': 0.38}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 300/773 [07:07<11:01,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4332, 'grad_norm': 0.2601514458656311, 'learning_rate': 0.0002, 'epoch': 0.39}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 310/773 [07:21<10:45,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4677, 'grad_norm': 0.26151296496391296, 'learning_rate': 0.0002, 'epoch': 0.4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 41%|████▏     | 320/773 [07:35<10:34,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4286, 'grad_norm': 0.23084235191345215, 'learning_rate': 0.0002, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 330/773 [07:49<10:22,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4615, 'grad_norm': 0.27504971623420715, 'learning_rate': 0.0002, 'epoch': 0.43}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 44%|████▍     | 340/773 [08:06<13:37,  1.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4276, 'grad_norm': 1.728877305984497, 'learning_rate': 0.0002, 'epoch': 0.44}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 350/773 [08:26<13:28,  1.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4871, 'grad_norm': 0.23188602924346924, 'learning_rate': 0.0002, 'epoch': 0.45}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 360/773 [08:45<13:25,  1.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4446, 'grad_norm': 0.2195141315460205, 'learning_rate': 0.0002, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 48%|████▊     | 370/773 [09:04<12:57,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4154, 'grad_norm': 0.2531881332397461, 'learning_rate': 0.0002, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 49%|████▉     | 380/773 [09:24<12:49,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4283, 'grad_norm': 0.2221730500459671, 'learning_rate': 0.0002, 'epoch': 0.49}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 390/773 [09:44<12:23,  1.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4645, 'grad_norm': 0.22982165217399597, 'learning_rate': 0.0002, 'epoch': 0.5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 400/773 [10:03<11:58,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4124, 'grad_norm': 0.23969420790672302, 'learning_rate': 0.0002, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 410/773 [10:23<12:05,  2.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.423, 'grad_norm': 0.22634534537792206, 'learning_rate': 0.0002, 'epoch': 0.53}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 54%|█████▍    | 420/773 [10:43<11:30,  1.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4172, 'grad_norm': 0.23348179459571838, 'learning_rate': 0.0002, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|█████▌    | 430/773 [11:03<11:18,  1.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4004, 'grad_norm': 0.2898707091808319, 'learning_rate': 0.0002, 'epoch': 0.56}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 440/773 [11:22<10:41,  1.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4128, 'grad_norm': 0.21427759528160095, 'learning_rate': 0.0002, 'epoch': 0.57}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 450/773 [11:38<07:56,  1.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3849, 'grad_norm': 0.22090360522270203, 'learning_rate': 0.0002, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|█████▉    | 460/773 [11:52<07:22,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4104, 'grad_norm': 0.2277952879667282, 'learning_rate': 0.0002, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 61%|██████    | 470/773 [12:06<07:05,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3993, 'grad_norm': 0.6363450288772583, 'learning_rate': 0.0002, 'epoch': 0.61}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 62%|██████▏   | 480/773 [12:20<06:54,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4298, 'grad_norm': 0.24585872888565063, 'learning_rate': 0.0002, 'epoch': 0.62}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 490/773 [12:35<06:38,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3822, 'grad_norm': 0.24537914991378784, 'learning_rate': 0.0002, 'epoch': 0.63}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▍   | 500/773 [12:49<06:23,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4125, 'grad_norm': 0.24289533495903015, 'learning_rate': 0.0002, 'epoch': 0.65}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 66%|██████▌   | 510/773 [13:03<06:07,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3833, 'grad_norm': 0.22519031167030334, 'learning_rate': 0.0002, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 520/773 [13:17<05:51,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4259, 'grad_norm': 0.24794025719165802, 'learning_rate': 0.0002, 'epoch': 0.67}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 69%|██████▊   | 530/773 [13:31<05:36,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4122, 'grad_norm': 0.22548390924930573, 'learning_rate': 0.0002, 'epoch': 0.69}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|██████▉   | 540/773 [13:45<05:22,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4221, 'grad_norm': 0.299169659614563, 'learning_rate': 0.0002, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 550/773 [13:58<05:11,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.395, 'grad_norm': 0.22573430836200714, 'learning_rate': 0.0002, 'epoch': 0.71}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 72%|███████▏  | 560/773 [14:12<05:00,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4075, 'grad_norm': 0.32023245096206665, 'learning_rate': 0.0002, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 74%|███████▎  | 570/773 [14:26<04:41,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.418, 'grad_norm': 0.603272557258606, 'learning_rate': 0.0002, 'epoch': 0.74}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 580/773 [14:40<04:29,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3923, 'grad_norm': 0.2487013339996338, 'learning_rate': 0.0002, 'epoch': 0.75}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 76%|███████▋  | 590/773 [14:54<04:13,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4169, 'grad_norm': 0.38712307810783386, 'learning_rate': 0.0002, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 78%|███████▊  | 600/773 [15:08<04:04,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3872, 'grad_norm': 0.24488097429275513, 'learning_rate': 0.0002, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 79%|███████▉  | 610/773 [15:22<03:49,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3677, 'grad_norm': 0.23598997294902802, 'learning_rate': 0.0002, 'epoch': 0.79}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 620/773 [15:36<03:34,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4042, 'grad_norm': 0.24547520279884338, 'learning_rate': 0.0002, 'epoch': 0.8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 82%|████████▏ | 630/773 [15:51<03:23,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3698, 'grad_norm': 0.23522484302520752, 'learning_rate': 0.0002, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 640/773 [16:05<03:07,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4039, 'grad_norm': 0.3357525169849396, 'learning_rate': 0.0002, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 84%|████████▍ | 650/773 [16:19<02:51,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3653, 'grad_norm': 0.28011077642440796, 'learning_rate': 0.0002, 'epoch': 0.84}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 660/773 [16:33<02:37,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4006, 'grad_norm': 0.2414141744375229, 'learning_rate': 0.0002, 'epoch': 0.85}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 670/773 [16:47<02:23,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3748, 'grad_norm': 1.5034081935882568, 'learning_rate': 0.0002, 'epoch': 0.87}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 88%|████████▊ | 680/773 [17:01<02:09,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3946, 'grad_norm': 0.24030402302742004, 'learning_rate': 0.0002, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 89%|████████▉ | 690/773 [17:15<01:56,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.383, 'grad_norm': 0.2691127061843872, 'learning_rate': 0.0002, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 91%|█████████ | 700/773 [17:29<01:42,  1.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4104, 'grad_norm': 0.4148443937301636, 'learning_rate': 0.0002, 'epoch': 0.91}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 92%|█████████▏| 710/773 [17:43<01:29,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3939, 'grad_norm': 0.23629221320152283, 'learning_rate': 0.0002, 'epoch': 0.92}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 720/773 [17:57<01:14,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3968, 'grad_norm': 0.3357713222503662, 'learning_rate': 0.0002, 'epoch': 0.93}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 730/773 [18:11<01:00,  1.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3584, 'grad_norm': 0.2280924916267395, 'learning_rate': 0.0002, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 740/773 [18:25<00:46,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.4074, 'grad_norm': 0.24406524002552032, 'learning_rate': 0.0002, 'epoch': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 750/773 [18:39<00:32,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3682, 'grad_norm': 0.22515447437763214, 'learning_rate': 0.0002, 'epoch': 0.97}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 760/773 [18:53<00:18,  1.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3828, 'grad_norm': 0.24991323053836823, 'learning_rate': 0.0002, 'epoch': 0.98}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████▉| 770/773 [19:07<00:04,  1.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3981, 'grad_norm': 0.26145321130752563, 'learning_rate': 0.0002, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 773/773 [19:21<00:00,  1.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 1161.4729, 'train_samples_per_second': 5.99, 'train_steps_per_second': 0.666, 'train_loss': 1.4427231576433466, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=773, training_loss=1.4427231576433466, metrics={'train_runtime': 1161.4729, 'train_samples_per_second': 5.99, 'train_steps_per_second': 0.666, 'total_flos': 9179569931157504.0, 'train_loss': 1.4427231576433466, 'epoch': 1.0})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# start training, the model will be automatically saved to the hub and the output directory\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_tokenizer\\\\tokenizer_config.json',\n",
       " './trained_tokenizer\\\\special_tokens_map.json',\n",
       " './trained_tokenizer\\\\vocab.json',\n",
       " './trained_tokenizer\\\\merges.txt',\n",
       " './trained_tokenizer\\\\added_tokens.json',\n",
       " './trained_tokenizer\\\\tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"./trained_model\")\n",
    "tokenizer.save_pretrained(\"./trained_tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import AutoPeftModelForCausalLM\n",
    "from transformers import AutoTokenizer, pipeline\n",
    " \n",
    "\"\"\" peft_model_id = \"./trained_model\"\n",
    "# peft_model_id = args.output_dir\n",
    " \n",
    "# Load Model with PEFT adapter\n",
    "model_2 = AutoPeftModelForCausalLM.from_pretrained(\n",
    "  peft_model_id,\n",
    "  device_map=\"auto\",\n",
    "  torch_dtype=torch.float16\n",
    ")\n",
    "tokenizer_2 = AutoTokenizer.from_pretrained(\"./trained_tokenizer\")\n",
    "# load into pipeline \"\"\"\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 6464 examples [00:00, 177561.09 examples/s]\n",
      "c:\\Users\\sabri\\Desktop\\GENAI Hackathon\\my_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:590: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sabri\\Desktop\\GENAI Hackathon\\my_env\\Lib\\site-packages\\transformers\\generation\\configuration_utils.py:595: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.1` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "c:\\Users\\sabri\\Desktop\\GENAI Hackathon\\my_env\\Lib\\site-packages\\torch\\utils\\checkpoint.py:87: UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "Just got back from downstairs, what a gross scene.\n",
      "Original Answer:\n",
      "Wow, what did you see?\n",
      "Generated Answer:\n",
      "Oh no! I hope it wasn't too much of a hassle for you?\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from random import randint\n",
    " \n",
    " \n",
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"Data/test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    " \n",
    "# Test on sample\n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    " \n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query:\n",
      "I stepped on my dog's tail accidentally earlier and I felt really bad about it.\n",
      "Original Answer:\n",
      "was he hurt?\n",
      "Generated Answer:\n",
      "Oh no! Did he get hurt?\n"
     ]
    }
   ],
   "source": [
    "# Load our test dataset\n",
    "eval_dataset = load_dataset(\"json\", data_files=\"Data/test_dataset.json\", split=\"train\")\n",
    "rand_idx = randint(0, len(eval_dataset))\n",
    " \n",
    "# Test on sample\n",
    "prompt = pipe.tokenizer.apply_chat_template(eval_dataset[rand_idx][\"messages\"][:2], tokenize=False, add_generation_prompt=True)\n",
    "outputs = pipe(prompt, max_new_tokens=256, do_sample=False, temperature=0.1, top_k=50, top_p=0.1, eos_token_id=pipe.tokenizer.eos_token_id, pad_token_id=pipe.tokenizer.pad_token_id)\n",
    " \n",
    "print(f\"Query:\\n{eval_dataset[rand_idx]['messages'][1]['content']}\")\n",
    "print(f\"Original Answer:\\n{eval_dataset[rand_idx]['messages'][2]['content']}\")\n",
    "print(f\"Generated Answer:\\n{outputs[0]['generated_text'][len(prompt):].strip()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
